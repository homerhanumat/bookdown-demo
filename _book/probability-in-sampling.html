<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Elementary Statistics with R</title>
  <meta content="text/html; charset=UTF-8" http-equiv="Content-Type">
  <meta name="description" content="These are course notes for MAT 111 at Georgetown College. They are greatly in need of revision.">
  <meta name="generator" content="bookdown 0.0.71 and GitBook 2.6.7">

  <meta property="og:title" content="Elementary Statistics with R" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="These are course notes for MAT 111 at Georgetown College. They are greatly in need of revision." />
  <meta name="github-repo" content="homerhanumat/bookdown-demo" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Elementary Statistics with R" />
  
  <meta name="twitter:description" content="These are course notes for MAT 111 at Georgetown College. They are greatly in need of revision." />
  

<meta name="author" content="Rebekah Robinson and Homer White">

<meta name="date" content="2016-05-18">

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="basic-probability.html">
<link rel="next" href="introduction-1.html">

<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>


  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Elementary Statistics With R</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Introduction</a></li>
<li class="chapter" data-level="2" data-path="describing-patterns-in-data.html"><a href="describing-patterns-in-data.html"><i class="fa fa-check"></i><b>2</b> Describing Patterns in Data</a></li>
<li class="chapter" data-level="3" data-path="relationships-between-two-factor-variables.html"><a href="relationships-between-two-factor-variables.html"><i class="fa fa-check"></i><b>3</b> Relationships Between Two Factor Variables</a></li>
<li class="chapter" data-level="4" data-path="relationships-between-two-numerical-variables.html"><a href="relationships-between-two-numerical-variables.html"><i class="fa fa-check"></i><b>4</b> Relationships Between Two Numerical Variables</a></li>
<li class="chapter" data-level="5" data-path="sampling-and-surveys.html"><a href="sampling-and-surveys.html"><i class="fa fa-check"></i><b>5</b> Sampling and Surveys</a></li>
<li class="chapter" data-level="6" data-path="design-of-studies.html"><a href="design-of-studies.html"><i class="fa fa-check"></i><b>6</b> Design of Studies</a></li>
<li class="chapter" data-level="7" data-path="basic-probability.html"><a href="basic-probability.html"><i class="fa fa-check"></i><b>7</b> Basic Probability</a></li>
<li class="chapter" data-level="8" data-path="probability-in-sampling.html"><a href="probability-in-sampling.html"><i class="fa fa-check"></i><b>8</b> Probability in Sampling</a><ul>
<li class="chapter" data-level="8.1" data-path="probability-in-sampling.html"><a href="probability-in-sampling.html#the-population-and-the-sample"><i class="fa fa-check"></i><b>8.1</b> The Population and the Sample</a><ul>
<li class="chapter" data-level="8.1.1" data-path="probability-in-sampling.html"><a href="probability-in-sampling.html#parameters"><i class="fa fa-check"></i><b>8.1.1</b> Parameters</a></li>
<li class="chapter" data-level="8.1.2" data-path="probability-in-sampling.html"><a href="probability-in-sampling.html#statistics"><i class="fa fa-check"></i><b>8.1.2</b> Statistics</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="probability-in-sampling.html"><a href="probability-in-sampling.html#from-questions-to-parameters"><i class="fa fa-check"></i><b>8.2</b> From Questions to Parameters</a><ul>
<li class="chapter" data-level="8.2.1" data-path="probability-in-sampling.html"><a href="probability-in-sampling.html#one-mean"><i class="fa fa-check"></i><b>8.2.1</b> One Mean</a></li>
<li class="chapter" data-level="8.2.2" data-path="probability-in-sampling.html"><a href="probability-in-sampling.html#one-proportion"><i class="fa fa-check"></i><b>8.2.2</b> One Proportion</a></li>
<li class="chapter" data-level="8.2.3" data-path="probability-in-sampling.html"><a href="probability-in-sampling.html#difference-of-two-means"><i class="fa fa-check"></i><b>8.2.3</b> Difference of Two Means</a></li>
<li class="chapter" data-level="8.2.4" data-path="probability-in-sampling.html"><a href="probability-in-sampling.html#difference-of-two-proportions"><i class="fa fa-check"></i><b>8.2.4</b> Difference of Two Proportions</a></li>
<li class="chapter" data-level="8.2.5" data-path="probability-in-sampling.html"><a href="probability-in-sampling.html#mean-of-differences"><i class="fa fa-check"></i><b>8.2.5</b> Mean of Differences</a></li>
<li class="chapter" data-level="8.2.6" data-path="probability-in-sampling.html"><a href="probability-in-sampling.html#the-basic-five-parameters"><i class="fa fa-check"></i><b>8.2.6</b> The “Basic Five” Parameters</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="probability-in-sampling.html"><a href="probability-in-sampling.html#parameters-in-experiments"><i class="fa fa-check"></i><b>8.3</b> Parameters in Experiments</a><ul>
<li class="chapter" data-level="8.3.1" data-path="probability-in-sampling.html"><a href="probability-in-sampling.html#anchoring-in-m111survey"><i class="fa fa-check"></i><b>8.3.1</b> Anchoring in m111survey</a></li>
<li class="chapter" data-level="8.3.2" data-path="probability-in-sampling.html"><a href="probability-in-sampling.html#knife-or-gun-again"><i class="fa fa-check"></i><b>8.3.2</b> Knife or Gun (Again)</a></li>
<li class="chapter" data-level="8.3.3" data-path="probability-in-sampling.html"><a href="probability-in-sampling.html#more-anchoring"><i class="fa fa-check"></i><b>8.3.3</b> More Anchoring</a></li>
</ul></li>
<li class="chapter" data-level="8.4" data-path="probability-in-sampling.html"><a href="probability-in-sampling.html#ev-and-sd-of-estimators"><i class="fa fa-check"></i><b>8.4</b> EV and SD of Estimators</a><ul>
<li class="chapter" data-level="8.4.1" data-path="probability-in-sampling.html"><a href="probability-in-sampling.html#estimating-one-mean"><i class="fa fa-check"></i><b>8.4.1</b> Estimating One Mean</a></li>
<li class="chapter" data-level="8.4.2" data-path="probability-in-sampling.html"><a href="probability-in-sampling.html#estimating-the-difference-of-two-means"><i class="fa fa-check"></i><b>8.4.2</b> Estimating the Difference of Two Means</a></li>
<li class="chapter" data-level="8.4.3" data-path="probability-in-sampling.html"><a href="probability-in-sampling.html#estimating-one-proportion"><i class="fa fa-check"></i><b>8.4.3</b> Estimating One Proportion</a></li>
<li class="chapter" data-level="8.4.4" data-path="probability-in-sampling.html"><a href="probability-in-sampling.html#estimating-the-difference-of-two-proportions"><i class="fa fa-check"></i><b>8.4.4</b> Estimating the Difference of Two Proportions</a></li>
<li class="chapter" data-level="8.4.5" data-path="probability-in-sampling.html"><a href="probability-in-sampling.html#estimating-the-mean-of-differences"><i class="fa fa-check"></i><b>8.4.5</b> Estimating the Mean of Differences</a></li>
<li class="chapter" data-level="8.4.6" data-path="probability-in-sampling.html"><a href="probability-in-sampling.html#properties-of-ev-and-sd-of-estimators"><i class="fa fa-check"></i><b>8.4.6</b> Properties of EV and SD of Estimators</a></li>
</ul></li>
<li class="chapter" data-level="8.5" data-path="probability-in-sampling.html"><a href="probability-in-sampling.html#estimators-shape-of-the-distribution"><i class="fa fa-check"></i><b>8.5</b> Estimators: Shape of the Distribution</a></li>
<li class="chapter" data-level="8.6" data-path="probability-in-sampling.html"><a href="probability-in-sampling.html#probability-for-estimators"><i class="fa fa-check"></i><b>8.6</b> Probability for Estimators</a></li>
<li class="chapter" data-level="8.7" data-path="probability-in-sampling.html"><a href="probability-in-sampling.html#inference-with-estimators"><i class="fa fa-check"></i><b>8.7</b> Inference With Estimators</a><ul>
<li class="chapter" data-level="8.7.1" data-path="probability-in-sampling.html"><a href="probability-in-sampling.html#we-are-not-deities"><i class="fa fa-check"></i><b>8.7.1</b> We Are Not Deities</a></li>
<li class="chapter" data-level="8.7.2" data-path="probability-in-sampling.html"><a href="probability-in-sampling.html#practice-with-estimator-and-se"><i class="fa fa-check"></i><b>8.7.2</b> Practice With Estimator and SE</a></li>
<li class="chapter" data-level="8.7.3" data-path="probability-in-sampling.html"><a href="probability-in-sampling.html#the-68-95-rule-for-estimation"><i class="fa fa-check"></i><b>8.7.3</b> The 68-95 Rule for Estimation</a></li>
</ul></li>
<li class="chapter" data-level="8.8" data-path="probability-in-sampling.html"><a href="probability-in-sampling.html#summary-of-formulas"><i class="fa fa-check"></i><b>8.8</b> Summary of Formulas</a></li>
<li class="chapter" data-level="8.9" data-path="probability-in-sampling.html"><a href="probability-in-sampling.html#thoughts-on-r"><i class="fa fa-check"></i><b>8.9</b> Thoughts on R</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="introduction-1.html"><a href="introduction-1.html"><i class="fa fa-check"></i><b>9</b> Introduction</a></li>
<li class="chapter" data-level="10" data-path="introduction-2.html"><a href="introduction-2.html"><i class="fa fa-check"></i><b>10</b> Introduction</a></li>
<li class="chapter" data-level="11" data-path="the-gamblers-die.html"><a href="the-gamblers-die.html"><i class="fa fa-check"></i><b>11</b> The Gambler’s Die</a></li>
<li class="chapter" data-level="12" data-path="chapter-2.html"><a href="chapter-2.html"><i class="fa fa-check"></i><b>12</b> Chapter 2</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Elementary Statistics with R</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="probability-in-sampling" class="section level1">
<h1><span class="header-section-number">Chapter 8</span> Probability in Sampling</h1>
<div id="the-population-and-the-sample" class="section level2">
<h2><span class="header-section-number">8.1</span> The Population and the Sample</h2>
<div id="parameters" class="section level3">
<h3><span class="header-section-number">8.1.1</span> Parameters</h3>
<p>We begin by recalling our imaginary population, consisting of 10,000 individuals:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">data</span>(imagpop)
<span class="kw">View</span>(imagpop)
<span class="kw">help</span>(imagpop)</code></pre></div>
<p>Let’s examine the <em>population distribution</em> of the variable <strong>height</strong> (see Figure [Imagpop Height]): the distribution appears to be roughly normal.</p>
<div class="figure"><span id="fig:imagpopheight"></span>
<img src="bookdown-demo_files/figure-html/imagpopheight-1.png" alt="Imagpop Height." width="672" />
<p class="caption">
Figure 8.1: Imagpop Height.
</p>
</div>
<p>From the population data, we can compute a wide variety of numbers. Numbers that are computed using the entire population are called <em>parameters</em>.</p>
<dl>
<dt>Parameter</dt>
<dd><p>A <em>parameter</em> is a number associated with a population.</p>
</dd>
</dl>
<p>Here are a few parameters for <code>imagpop</code>:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">favstats</span>(~height,<span class="dt">data=</span>imagpop)</code></pre></div>
<pre><code>##   min   Q1 median   Q3  max     mean       sd     n missing
##  53.8 64.7   67.5 70.3 80.2 67.53012 3.907014 10000       0</code></pre>
<p>When we apply <code>favstats()</code> to a population, everything it returns is a parameter. We have such things as:</p>
<ul>
<li>the population mean, written <span class="math inline">\(\mu\)</span> or just “mu”. Its numerical value is 67.53012.</li>
<li>the population median, usually just written as <span class="math inline">\(m\)</span>. Its numerical value is 67.5.</li>
<li>the population standard deviation, written <span class="math inline">\(\sigma\)</span>, or just “sigma”. Its numerical value is 3.9070139.</li>
</ul>
<p>Let’s look at another variable from <code>imagpop</code>, namely the categorical variable <strong>sex</strong>:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">xtabs</span>(~sex,<span class="dt">data=</span>imagpop)</code></pre></div>
<pre><code>## sex
## female   male 
##   4968   5032</code></pre>
<p>We see that, of the 10,000 members of the population, 5032—or 50.32%—are male. The 50.32% figure is the <em>population percentage</em> of males; we also say that the <em>population proportion</em> of males is 0.5032. Both of these quantities are parameters, since they are numbers associated with the population.</p>
<p>If you are lucky enough to have information on all the entire population, as we do with <code>imagpop</code>, then you can see a lot of parameters at once by calling the <code>summary()</code> function:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">summary</span>(imagpop)</code></pre></div>
<p>We show below only the results for the first four variables in <code>imagpop</code>:</p>
<pre><code>##      sex        math          income          cappun    
##  female:4968   no :9537   Min.   :   200   favor :2976  
##  male  :5032   yes: 463   1st Qu.: 19300   oppose:7024  
##                           Median : 33600                
##                           Mean   : 40317                
##                           3rd Qu.: 54100                
##                           Max.   :262200</code></pre>
</div>
<div id="statistics" class="section level3">
<h3><span class="header-section-number">8.1.2</span> Statistics</h3>
<p>We have learned that a <em>parameter</em> is a number associated with a population. Usually we would like very much to know the numerical value of one or more parameters, but for practical reasons we are unable to examine every member of the population in order to compute these parameters. Usually the best we can do is to take a random sample from the population, and compute numbers based on that sample. Such numbers are called <em>statistics</em>.</p>
<dl>
<dt>Statistic</dt>
<dd><p>A <em>statistic</em> is a number that we can compute from sample data.</p>
</dd>
</dl>
<p>Usually, we are interested in computing statistics that might serve to <em>estimate</em> some parameter.</p>
<p>Let’s do an example. Suppose that we only have time to take a sample of 10 members from <code>imagpop</code>. The function <code>popsamp()</code> will take a simple random sample from any given data frame, of any given size:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">popsamp</span>(<span class="dt">n=</span><span class="dv">10</span>,<span class="dt">pop=</span>imagpop)</code></pre></div>
<pre><code>##         sex math income cappun height idealheight diff kkardashtemp
## 7210 female   no  39700 oppose   65.6          67  1.4            2
## 8757 female   no  81000  favor   64.3          66  1.7            4
## 7609   male   no  35100  favor   70.9          74  3.1           78
## 8859   male   no  35900 oppose   70.4          73  2.6           80
## 4563   male   no  16100 oppose   73.5          76  2.5           80
## 1663 female   no  66600 oppose   69.0          71  2.0           10
## 3250   male   no  37900 oppose   63.1          65  1.9           99
## 5089   male   no  77300 oppose   68.4          71  2.6           93
## 7272   male   no  62600  favor   70.4          73  2.6           96
## 9889 female   no  59300 oppose   59.0          61  2.0            3</code></pre>
<p>If you run the function several times, you will most likely get a different sample each time.</p>
<p>Now let’s compute some statistics from a sample: this time, let’s sample n = 100 members from the population by simple random sampling. We’ll use the summary function to compute a lot of statistics at once (but we’ll show the output just for the first few variables:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">summary</span>(<span class="kw">popsamp</span>(<span class="dt">n=</span><span class="dv">100</span>,<span class="dt">pop=</span>imagpop))</code></pre></div>
<pre><code>##      sex      math        income          cappun  
##  female:49   no :91   Min.   :   400   favor :25  
##  male  :51   yes: 9   1st Qu.: 14625   oppose:75  
##                       Median : 28350              
##                       Mean   : 36676              
##                       3rd Qu.: 48325              
##                       Max.   :207000</code></pre>
<p>If you try it yourself a few times, you will see that that the statistics vary, from sample to sample. This makes, sense, because the sample itself is random. In fact, a statistic is always a <em>random variable</em>!</p>
<p>You can practice more with the idea of a statistic as a random variable by playing again with the app <code>SimpleRandom()</code>, introduced in Chapter 5:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">require</span>(manipulate)
<span class="kw">SimpleRandom</span>()</code></pre></div>
<p>For any variable you selected, you see that the statistics hop around from sample to sample, but on the whole each statistic more or less resembles the parameter in the table one row above it. In fact, that’s why we compute statistics:</p>
<blockquote>
<p><strong>What Statistics Are For</strong>: We use statistics to estimate parameters.</p>
</blockquote>
<p>Used in this way, a statistic is called a <em>point estimate</em> for the parameter it is used to estimate.</p>
<dl>
<dt>Point Estimator</dt>
<dd><p>A <em>point estimator</em> for a population parameter is a statistic that is used to estimate the parameter.</p>
</dd>
</dl>
<p>Even though we know it is highly unlikely to be exactly equal to the parameter, a well-chosen point estimator constitutes our single “best guess” as to the value of the parameter in question.</p>
<p>To summarize:</p>
<ul>
<li>A parameter is a number associated with a population.
<ul>
<li>We regard it as fixed (at least at some fixed time.)</li>
<li>However, it is usually unknown.</li>
</ul></li>
<li>A statistic is a number we can compute from a sample.
<ul>
<li>We can know its value, because we can compute it from our sample.</li>
<li>But it varies from sample to sample.</li>
</ul></li>
<li>We use statistics to estimate parameters.</li>
</ul>
</div>
</div>
<div id="from-questions-to-parameters" class="section level2">
<h2><span class="header-section-number">8.2</span> From Questions to Parameters</h2>
<p>Often—not always, but quite often—a Research Question can be construed as a question about the value of one or more population parameters. If you can learn how to turn Research Questions into questions about the value of parameters, then you gain access to an array of powerful statistical techniques.</p>
<p>We will learn how to turn Research Questions into questions about parameters by trying out lots of examples. To start, we’ll keep these examples focused on <code>imagpop</code>.</p>
<p>Consider the following</p>
<blockquote>
<p><strong>Research Question</strong>: We would like to know the center and spread of height in the population.</p>
</blockquote>
<p>From the statement of the Research Question, we see that we are interested in:</p>
<ul>
<li><span class="math inline">\(\mu\)</span> = mean height of all people in <code>imagpop</code>, and</li>
<li><span class="math inline">\(\sigma\)</span> = the standard deviation of all people in <code>imagpop</code>.</li>
</ul>
<p>One could also construe this Research Question as a question about population median, and population IQR. As a rule, though, we will go with mean and SD as a first preference.</p>
<p>We next consider a sequence of Research Questions that connect to a standard set of parameters. These parameters crop up so frequently that each will be accorded its own separate sub-section in this Chapter.</p>
<div id="one-mean" class="section level3">
<h3><span class="header-section-number">8.2.1</span> One Mean</h3>
<blockquote>
<p><strong>Research Question</strong>: We would like to know the mean rating that people in <code>imagpop</code> give to the celebrity Kim Kardashian.</p>
</blockquote>
<p>In this example, the parameter of interest is pretty obvious. We are interested in knowing:</p>
<blockquote>
<p><span class="math inline">\(\mu\)</span> = mean Kim Kardashian temperature rating for <code>imagpop</code>.</p>
</blockquote>
</div>
<div id="one-proportion" class="section level3">
<h3><span class="header-section-number">8.2.2</span> One Proportion</h3>
<blockquote>
<p><strong>Research Question</strong>: We would like to know the proportion of people in the population who majored in math.</p>
</blockquote>
<p>Again the parameter of interest is fairly obvious. We are interested in knowing:</p>
<blockquote>
<p><span class="math inline">\(p\)</span> = the proportion of all persons in <code>imagpop</code> who majored in math.</p>
</blockquote>
</div>
<div id="difference-of-two-means" class="section level3">
<h3><span class="header-section-number">8.2.3</span> Difference of Two Means</h3>
<blockquote>
<p><strong>Research Question</strong>: We wonder how much taller guys are, on average, than gals.</p>
</blockquote>
<p>This time, we break <code>imagpop</code> into two separate populations: all of the females, and all of the males. The parameters we are interested in are:</p>
<blockquote>
<p><span class="math inline">\(\mu_1\)</span> = mean height of all males in <code>imagpop</code></p>
</blockquote>
<p>and</p>
<blockquote>
<p><span class="math inline">\(\mu_2\)</span> = mean height of all females in <code>imagpop</code>.</p>
</blockquote>
<p>Wanting to know how much taller the guys are, on average, is the same thing as wanting to know the <em>difference of means</em> <span class="math inline">\(\mu_1 - \mu_2\)</span>. This becomes our parameter of interest.</p>
<p>Here is another example. Suppose we are interested in the following</p>
<blockquote>
<p><strong>Research Question</strong>: Do math majors make more money, in average, than non-math majors do?</p>
</blockquote>
<p>Again, we break <code>imagpop</code> into two populations: all of the math majors, and all of the non-math majors. The parameters we are interested in are:</p>
<blockquote>
<p><span class="math inline">\(\mu_1\)</span> = mean annual income for all math majors in <code>imagpop</code></p>
</blockquote>
<p>and</p>
<blockquote>
<p><span class="math inline">\(\mu_2\)</span> = mean annual income for all non-math majors in <code>imagpop</code>.</p>
</blockquote>
<p>Wanting to know whether math majors make more money is the same as wanting to know whether the difference of means, <span class="math inline">\(\mu_1 - \mu_2\)</span>, is positive.</p>
<p>Watch out, though, for the following type of question:</p>
<blockquote>
<p><strong>Research Question</strong>: It is known that the mean height for the population of Australia is 67 inches. Do people in <code>imagpop</code> have the same height, on average?</p>
</blockquote>
<p>You might think that this is another question about the difference of two means. However, we <em>already know</em> the mean height of all Australians, so really we are interested in just one population mean:</p>
<blockquote>
<p><span class="math inline">\(\mu\)</span> = mean height of all people in <code>imagpop</code>.</p>
</blockquote>
<p>We just want to know whether or not <span class="math inline">\(\mu = 67\)</span>.</p>
<p>In order to be in a “difference of two means” situation, we should be dealing with two populations, and we should not know the mean of either population. Also, our plan should be to take two independent samples, one from each population, and to estimate each of the means from these samples.</p>
<p>(<strong>Note</strong>: “Independent” means that the samples don’t have anything to do with each other: knowing who is in one sample tells you nothing about who is in the other sample.)</p>
</div>
<div id="difference-of-two-proportions" class="section level3">
<h3><span class="header-section-number">8.2.4</span> Difference of Two Proportions</h3>
<p>Suppose we are interested in the following</p>
<blockquote>
<p><strong>Research Question</strong>: Who is more likely to favor capital punishment: a guy or a gal?</p>
</blockquote>
<p>This one is tricky for many students; the key is to interpret “likelihood” as probability. Probabilities are between 0 and 1, so they are proportions. Hence the question is really asking:</p>
<blockquote>
<p>Which is bigger: the proportion of all males who favor capital punishment, or the proportion of all females who favor capital capital punishment?</p>
</blockquote>
<p>When you ask the question this way, you see that it is a question about two populations:</p>
<ul>
<li>the population of all males, and</li>
<li>the population of all females.</li>
</ul>
<p>Within each population, some proportion of people favor capital punishment, and we want to know which of the two proportions is larger. So are parameters of interest are:</p>
<blockquote>
<p><span class="math inline">\(p_1\)</span> = the proportion of all males in <code>imagpop</code> who favor capital punishment;</p>
</blockquote>
<p>and</p>
<blockquote>
<p><span class="math inline">\(p_2\)</span> = the proportion of all females in <code>imagpop</code> who favor capital punishment.</p>
</blockquote>
<p>We are interested in the difference of these two proportions: <span class="math inline">\(p_1-p_2\)</span>.</p>
</div>
<div id="mean-of-differences" class="section level3">
<h3><span class="header-section-number">8.2.5</span> Mean of Differences</h3>
<p>For our next research question, we will move back to the familiar <code>mat111survey</code> data. The population from which this sample was drawn is, of course, the population of all GC students.</p>
<blockquote>
<p><strong>Research Quetion</strong>: Do people at Georgetown want to be taller than they actually are?</p>
</blockquote>
<p>This is another tricky one. Many people say: “there are two populations means at issue here: the mean actual height of the population, and the mean ideal height of the population. We want to know if the second mean is larger than the first. Hence we are interested in the difference of two means.”</p>
<p>This isn’t quite right. Recall that in order to be dealing with the difference of two means, we need to be in the situation of taking two independent samples from two populations. But this time we have one sample, from one population: it’s just that we did a repeated-measures study by asking each individual two questions—actual height and ideal height.</p>
<p>Remember that in a repeated-measures study, you focus on the <em>difference</em> between the two measurements. Hence we are actually interested in the following parameter:</p>
<blockquote>
<p><span class="math inline">\(\mu_d\)</span> = the mean of the difference between ideal height and actual height, for all students at Georgetown College.</p>
</blockquote>
<p>For another example, let’s look back at the labels-and-perception experiment:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">data</span>(labels)
<span class="kw">View</span>(labels)
<span class="kw">help</span>(labels)</code></pre></div>
<p>In connection with the <code>labels</code> data, we are probably interested in the following</p>
<blockquote>
<p><strong>Research Question</strong>: On average, which label results in the higher rating for the peanut butter: Jiff or Great Value?</p>
</blockquote>
<p>This was a repeated measures design, and we are interested in the difference between the two ratings. So we are interested in:</p>
<blockquote>
<p><span class="math inline">\(\mu_d\)</span> = mean difference in ratings (Jiff rating - GC rating) for all Georgetown College students.</p>
</blockquote>
<p>Notice that we were careful to make the parameter refer to the entire population from which the sample of 30 students was drawn. <strong>A parameter is a numerical feature of a population!</strong></p>
<p>In general, research questions associated with matched-pair and repeated-measure studies are likely to turn into questions about a mean of differences.</p>
<blockquote>
<p>Practice: For each of the following Research Questions, define the parameter(s) of interest, and then restate the Research Question in terms of that (or those) parameters.</p>
</blockquote>
<blockquote>
<ol style="list-style-type: decimal">
<li>(For the <code>m111survey</code> data.) Research Question: UK students are known to have a mean GPA of 3.0. Is the mean GPA of all Georgetown College students higher than the mean at UK?</li>
</ol>
</blockquote>
<blockquote>
<ol start="2" style="list-style-type: decimal">
<li>(For the <code>m111survey</code> data.) Research Question: Who drives faster on average: Georgetown College males or GC females?</li>
</ol>
</blockquote>
<blockquote>
<ol start="3" style="list-style-type: decimal">
<li>(For the <code>m111survey</code> data.) Research Question: Do a majority of GC males believe in love at first sight?</li>
</ol>
</blockquote>
<blockquote>
<ol start="4" style="list-style-type: decimal">
<li>(For the <code>m111survey</code> data.) Research Question: Who is more likely to believe in love at first sight: A GC male or a GC female?</li>
</ol>
</blockquote>
<blockquote>
<ol start="5" style="list-style-type: decimal">
<li>(For the <code>m111survey</code> data.) Research Question: Who wants to increase their height more, on average: GC males or GC females? (Think carefully about this one!)</li>
</ol>
</blockquote>
</div>
<div id="the-basic-five-parameters" class="section level3">
<h3><span class="header-section-number">8.2.6</span> The “Basic Five” Parameters</h3>
<p>At the elementary level there are five types of Research Question that come up so frequently that the parameters associated with them are called the Basic Five:</p>
<ol style="list-style-type: decimal">
<li>One Mean (<span class="math inline">\(\mu\)</span>)</li>
<li>One Proportion (<span class="math inline">\(p\)</span>)</li>
<li>Difference of Two Means (<span class="math inline">\(\mu_1-\mu_2\)</span>)</li>
<li>Difference of Two Proportions (<span class="math inline">\(p_1-p_2\)</span>)</li>
<li>Mean of Differences (<span class="math inline">\(\mu_d\)</span>)</li>
</ol>
<p>We have already seen one or more examples of each of the Basic Five, and we’ll see many more examples in the future.</p>
</div>
</div>
<div id="parameters-in-experiments" class="section level2">
<h2><span class="header-section-number">8.3</span> Parameters in Experiments</h2>
<p>Most of the examples of Research Questions that we considered in the previous section were based on observational studies, in which the data was considered to be a sample from some larger population. Because of the issues involved in obtaining consent for inclusion in an experiment, we may or may not be able to consider the subjects in an experiment as a random sample from some larger population, and even when we can, we often speak about that population a bit differently than we do for observational studies.</p>
<p>Let’s consider a couple of examples.</p>
<div id="anchoring-in-m111survey" class="section level3">
<h3><span class="header-section-number">8.3.1</span> Anchoring in m111survey</h3>
<p>To begin with, look at <code>m111surveyfa12</code>:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">data</span>(m111surveyfa12)
<span class="kw">View</span>(m111surveyfa12)
<span class="kw">help</span>(m111surveyfa12)</code></pre></div>
<p>Most of the Research Questions associated with this data frame may be considered as arising from an observational study. For example, the Research Question: “who drives faster, on average a male or female?” is based on an observational study, since the explanatory variable here is <strong>sex</strong>, and the values of <strong>sex</strong> cannot be assigned to subjects by researchers.</p>
<p>On the other hand, consider the question about the population of Canada. When subjects filled out their survey forms, some of the subjects were looking at forms where the question about Canada was stated as follows:</p>
<blockquote>
<p>“The population of Australia is about 23 million. What do you think is the population of Canada? (Give your answer in millions.)”</p>
</blockquote>
<p>The rest of the subjects were looking at forms where the question about Canada was stated as follows:</p>
<blockquote>
<p>“The population of the United States is about 312 million. What do you think is the population of Canada? (Give your answer in millions.)”</p>
</blockquote>
<p>The country whose population is given is called an <em>anchor</em>. Behavioral psychologists tell us that anchors can affect the way we think about that question, even when the anchor has no logical bearing on the question itself.</p>
<p>If everybody processes information in a completely rational way, then one’s answer about Canada would be the same, no matter whether one is told first about Australia or about the United States. The “Canada” question was not asked because we were interested in how well Georgetown College students know geography trivia: we were actually interested in the question of whether Georgetown College students process information rationally! In other words, the Research Question was:</p>
<blockquote>
<p><strong>Research Question</strong>: Who gives a higher estimate, on average, for the population of Canada: a person who was first told the population of the United States, or a person who was first told the population of Australia?</p>
</blockquote>
<p>In this question, the explanatory variable is <strong>anchor</strong> (the type of form) and the response variable is <strong>canada</strong>. Since researchers were able to assign forms to subjects (for the most part they attempted to do so randomly), they were performing an experiment.</p>
<p>Here is how we translate the Research Question into a question about parameters. The parameters of interest are:</p>
<blockquote>
<p><span class="math inline">\(\mu_1\)</span> = the mean estimate of the population of Canada given by all GC students, if all of them could have been given a form in which they are first told the population of Australia.</p>
</blockquote>
<blockquote>
<p><span class="math inline">\(\mu_2\)</span> = the mean estimate of the population of Canada given by all GC students, if all of them could have been given a form in which they are first told the population of the United States.</p>
</blockquote>
<p>The Research Question turns into a question about whether the difference of means, <span class="math inline">\(\mu_1 - \mu_2\)</span>, is zero or not. A difference of zero would indicate that, on average, the anchor made no difference in the response.</p>
<p>We have met several examples before of the difference of two means. The previous examples, though, were based on observational studies, and in that case there were two populations, and the plan was to take two independent simple random samples separately from the two populations.</p>
<p>In this experiment, there is just one population: all Georgetown College students. We simply imagine that the population is treated in two different ways:</p>
<ul>
<li>everyone in the population answers the Canada question after learning about Australia. The mean of their answers is <span class="math inline">\(\mu_1\)</span>.</li>
<li>everyone in the population answers the Canada question after learning about the United States. The mean of their answers is <span class="math inline">\(\mu_2\)</span>.</li>
</ul>
<p>By taking one sample from the population—the MAT 111 students who took the survey—and breaking them into two treatment groups (students with the Australia form and students with the U.S. form) we obtained two samples: one from each imaginary population. The samples aren’t really independent (if George is given an Australia form then you know for sure that he will not be picked to get a U.S. form), but that’s how it is with experiments.</p>
</div>
<div id="knife-or-gun-again" class="section level3">
<h3><span class="header-section-number">8.3.2</span> Knife or Gun (Again)</h3>
<p>Let’s look at another example. Recall the Knife or Gun study:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">data</span>(knifeorgunblock)
<span class="kw">View</span>(knifeorgunblock)
<span class="kw">help</span>(knifeorgunblock)</code></pre></div>
<p>Remember that we were interested in the</p>
<blockquote>
<p><strong>Research Question</strong>: What makes a person yell louder: being killed with a knife or being killed with a gun?</p>
</blockquote>
<p>For this experiment, we know in advance that the results probably don’t apply to some larger population, because anyone who would agree to be part of an experiment in which they get killed is liable to be extremely different from the general population—perhaps in ways associated with how they respond when being attacked with knives and guns! This fact affects how we define the parameters of interest. In this case, the parameters are defined as follows:</p>
<blockquote>
<p><span class="math inline">\(\mu_1\)</span> = the mean volume of yells for all 20 subjects, if they could have all been killed with a knife.</p>
</blockquote>
<blockquote>
<p><span class="math inline">\(\mu_2\)</span> = the mean volume of yells for all 20 subjects, if they could have all been killed with a gun.</p>
</blockquote>
<p>This time the so-called population is just the set of subjects themselves, not some larger group out of which the subjects are a sample. Just as in the previous example, though, we do imagine that this set of subjects is treated in two different ways, so we still have two “imaginary” populations.</p>
<p>You should define your parameters in this more restrictive way whenever the subjects in your experiments cannot be considered as representative of some larger population.</p>
</div>
<div id="more-anchoring" class="section level3">
<h3><span class="header-section-number">8.3.3</span> More Anchoring</h3>
<p>In both of the examples above, the experiment had two treatment groups and the response variable was numerical. This resulted in a <em>difference of two means</em> situation. If your experiment has two groups and the response variable is categorical with two values (e.g., “yes” or “no”), then you will end up being interested in the difference of two proportions.</p>
<p>As an example, consider the attitudes experiment:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">data</span>(attitudes)
<span class="kw">View</span>(attitudes)
<span class="kw">help</span>(attitudes)</code></pre></div>
<p>The question about spending habits involved an experiment: some of the subjects were looking at a form in $20 was lost simply by the money falling out of a purse or wallet somehow. Other subjects faced a scenario in which they had lost $20 by purchasing a ticket and then losing that ticket. Either way, the subject is down by $20, so if everyone makes financial decisions on a purely rational basis, then one’s decision about whether to attend the rock concert anyway should be unaffected by what form one is looking at: a person who would elect to attend the concert after having lost money would also elect to attend after having lost a ticket, and vice versa.</p>
<p>Obviously we are interested in the same sort of behavioral psychology question as in the earlier anchor experiment on guessing populations:</p>
<blockquote>
<p><strong>Research Question</strong>: Who is more likely to elect to attend the rock concert: a person who has lost $20 in cash, or a person who has lost a $20 ticket?</p>
</blockquote>
<p>The response variable is the categorical variable <strong>conc.dec</strong>, which has two values (“buy” and “not.buy”), so the parameters of interest must be proportions:</p>
<blockquote>
<p><span class="math inline">\(p_1\)</span> = the proportion of all GC students who would elect to attend the rock concert, if all of them could be given a survey form describing a scenario where they have lost $20 in cash.</p>
</blockquote>
<blockquote>
<p><span class="math inline">\(p_2\)</span> = the proportion of all GC students who would elect to attend the rock concert, if all of them could be given a survey form describing a scenario where they have lost a $20 ticket.</p>
</blockquote>
<p>We are interested in the difference of two proportions <span class="math inline">\(p_1-p_2\)</span>. If this difference is 0, then the way loses one’s money has no effect on the likelihood of whether one will buy a ticket anyway.</p>
<blockquote>
<p>Practice. Consider the <strong>attitudes</strong> dataset, and the Research Question: <em>On average, does the suggested race of the defendant affect the length of sentence that would be recommended by a Georgetown College student?</em> Define appropriate parameters and translate the Research Question into a question about these parameters. Which one of the Basic Five is represented here?</p>
</blockquote>
</div>
</div>
<div id="ev-and-sd-of-estimators" class="section level2">
<h2><span class="header-section-number">8.4</span> EV and SD of Estimators</h2>
<p>As we said earlier, people like to use a statistic – a random variable that is computed from a sample – to estimate a parameter. Let’s talk about the statistics that are used to estimate the Basic Five parameters.</p>
<div id="estimating-one-mean" class="section level3">
<h3><span class="header-section-number">8.4.1</span> Estimating One Mean</h3>
<p>To estimate one mean, <span class="math inline">\(\mu\)</span>, we take a simple random sample and compute the <em>sample mean</em>, which is written <span class="math inline">\(\bar{x}\)</span>. As we know from previous chapters,</p>
<blockquote>
<p><span class="math display">\[\bar{x} =\frac{\sum x_i}{n},\]</span></p>
</blockquote>
<p>which is to say that it is the sum of the values in the sample, divided by the sample size.</p>
<p>This makes good sense. After all, the population mean <span class="math inline">\(\mu\)</span> is the sum of all of the values in the population, divided by the number of individuals in the population, and a random sample is (hopefully) a fair representation of the population. So it seems we should estimate the mean of the population by calculating the mean of the sample.</p>
<p>You might wonder how good a job <span class="math inline">\(\bar{x}\)</span> does, as an estimator of <span class="math inline">\(\mu\)</span>. Statisticians have studied this question, and the short answer is: “It does a very good job indeed!” A slightly more detailed answer starts by pointing out that <span class="math inline">\(\bar{x}\)</span> is a random variable—it is, after all, a number that depends on chance—so it has an expected value (EV for short) and a standard deviation (SD).</p>
<p>Recall from previous chapters that the EV of a random variable is what you expect to get, on average, in many “tries” of the random variable. Statisticians have proven that</p>
<p><span class="math display">\[EV(\bar{x})=\mu.\]</span></p>
<p>Recall also that the SD of a random variable is about how much the random variable is liable to differ from it’s EV. Statisticians have proven that the SD of <span class="math inline">\(\bar{x}\)</span> is:</p>
<p><span class="math display">\[SD(\bar{x}) =\frac{\sigma}{\sqrt{n}},\]</span></p>
<p>where <span class="math inline">\(\sigma\)</span> is the SD of the population, and <span class="math inline">\(n\)</span> is the size of the sample.</p>
<p>Always keep in mind how EV and SD work together to describe how a random variable is liable to turn out. Now that we know the formulas for EV and SD of <span class="math inline">\(\bar{x}\)</span>, we can say:</p>
<blockquote>
<p>When you take a simple random sample of size <span class="math inline">\(n\)</span> from a population, the sample mean <span class="math inline">\(\bar{x}\)</span> is liable to be about <span class="math inline">\(\mu\)</span>, give or take <span class="math inline">\(\frac{\sigma}{\sqrt{n}}\)</span> or so.</p>
</blockquote>
</div>
<div id="estimating-the-difference-of-two-means" class="section level3">
<h3><span class="header-section-number">8.4.2</span> Estimating the Difference of Two Means</h3>
<p>When you want to estimate the difference <span class="math inline">\(\mu_1-\mu_2\)</span> between the means of two populations, you could take a simple random sample from each population and compute:</p>
<ul>
<li><span class="math inline">\(\bar{x}_1\)</span>, the mean of the sample from the first population, as well as</li>
<li><span class="math inline">\(\bar{x}_2\)</span>, the mean of the sample from the second population.</li>
</ul>
<p>You could then subtract these two sample means. The difference, <span class="math inline">\(\bar{x}_1-\bar{x}_2\)</span>, would be an estimator for <span class="math inline">\(\mu_1-\mu_2\)</span>.</p>
<p>Statisticians have shown that the EV of <span class="math inline">\(\bar{x}_1-\bar{x}_2\)</span> is:</p>
<p><span class="math display">\[EV(\bar{x}_1-\bar{x}_2)=\mu_1-\mu_2.\]</span></p>
<p>The SD of <span class="math inline">\(\bar{x}_1-\bar{x}_2\)</span> is:</p>
<p><span class="math display">\[SD(\bar{x}_1-\bar{x}_2)=\sqrt{\frac{\sigma_1^2}{n_1}+\frac{\sigma_2^2}{n_2}}.\]</span></p>
<p>This is a fairly complicated-looking quantity. Let’s work with a numerical example:</p>
<blockquote>
<p><strong>Example:</strong> Suppose that the population of all GC males has a mean height of 71 inches, with a standard deviation of 3 inches. Suppose also that the population of all GC females has a mean height of 68 inches, with a standard deviation of 2.5 inches. You plan to take a simple random sample of 25 males and an independent simple random sample of 36 females. You plan to estimate <span class="math inline">\(\mu_1-\mu_2\)</span>, the mean GC male height minus the mean GC female height, by <span class="math inline">\(\bar{x}_1-\bar{x}_2\)</span>, the difference of your sample means.</p>
</blockquote>
<blockquote>
<p>(1). About what do you expect <span class="math inline">\(\bar{x}_1-\bar{x}_2\)</span> to work out to be?</p>
</blockquote>
<blockquote>
<p>(2). Give or take about how much?</p>
</blockquote>
<p>As for the first question, we expect <span class="math inline">\(\bar{x}_1-\bar{x}_2\)</span> to work out to about its EV, which is:</p>
<p><span class="math display">\[\mu_1-\mu_2 = 71-68 = 3\]</span></p>
<p>inches.</p>
<p>For the second question we need to compute the SD, and we might as well have R do the work for us. From the given information, we seen that:</p>
<ul>
<li><span class="math inline">\(\sigma_1=3\)</span></li>
<li><span class="math inline">\(\sigma_2=2.5\)</span></li>
<li><span class="math inline">\(n_1=25\)</span></li>
<li><span class="math inline">\(n_2=36\)</span></li>
</ul>
<p>We then plug these values into the formula for <span class="math inline">\(SD(\bar{x}_1-\bar{x}_2)\)</span>, using R as a calculator to do the arithmetic for us:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">sqrt</span>(<span class="dv">3</span>^<span class="dv">2</span>/<span class="dv">25</span><span class="fl">+2.5</span>^<span class="dv">2</span>/<span class="dv">36</span>)</code></pre></div>
<pre><code>## [1] 0.7304869</code></pre>
<p>So we expect <span class="math inline">\(\bar{x}_1-\bar{x}_2\)</span> to work out to about 3 inches, give or take 0.73 inches or so.</p>
</div>
<div id="estimating-one-proportion" class="section level3">
<h3><span class="header-section-number">8.4.3</span> Estimating One Proportion</h3>
<p>When you need to estimate a population proportion <span class="math inline">\(p\)</span>, you could take a simple random sample (let’s abbreviate this as SRS, shall we?) and count the number <span class="math inline">\(X\)</span> of individuals in the sample who possess the characteristic of interest. You could then divide <span class="math inline">\(X\)</span> by <span class="math inline">\(n\)</span>, the size of the sample, to get a proportion. This is called – naturally enough – the <em>sample proportion</em>, and it is written symbolically as <span class="math inline">\(\hat{p}\)</span>. So the sample proportion is:</p>
<p><span class="math display">\[\hat{p}=\frac{X}{n},\]</span></p>
<p>and it is used to estimate the parameter <span class="math inline">\(p\)</span>. Roughly, you can think of the sample proportion as: “the number of yesses, divided by the number of people you asked.” The population proportion can be thought of as: “the number of yesses in the population, divided by the population size.”</p>
<p>The EV of <span class="math inline">\(\hat{p}\)</span> is just <span class="math inline">\(p\)</span>, the population proportion. The SD of <span class="math inline">\(\hat{p}\)</span> is:</p>
<p><span class="math display">\[SD(\hat{p})=\sqrt{\frac{p(1-p)}{n}},\]</span></p>
<p>where <span class="math inline">\(n\)</span> is again the sample size.</p>
<p>Here is an example of the use of these formulas.</p>
<blockquote>
<p><strong>Example:</strong> 40% of the individuals in a certain population think that marijuana use should be legal. A social scientist, who does not know this fact, would like to estimate the percentage of folks who think marijuana should be legal. She plans to take a SRS of 400 individuals, and compute the proportion of the sample who think marijuana should be legal. Fill in the blanks: “Her estimate is liable to be around ______ %, give or take ______ % or so.”</p>
</blockquote>
<p><strong>Answer</strong>: The EV of <span class="math inline">\(\hat{p}\)</span> is <span class="math inline">\(p\)</span>, which in this case is 0.40. Hence the first blank should be filled in with 40 (percent). The second blank should contain the SD of <span class="math inline">\(\hat{p}\)</span>, multiplied by 100 to make it a percentage. Again we can just use R as a calculator:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">sqrt</span>(.<span class="dv">4</span>*(<span class="dv">1</span><span class="fl">-.4</span>)/<span class="dv">400</span>)*<span class="dv">100</span></code></pre></div>
<pre><code>## [1] 2.44949</code></pre>
<p>So we should fill in the second blank with 2.45 (percent).</p>
</div>
<div id="estimating-the-difference-of-two-proportions" class="section level3">
<h3><span class="header-section-number">8.4.4</span> Estimating the Difference of Two Proportions</h3>
<p>When you need to estimate the difference between two population proportions <span class="math inline">\(p_1\)</span> and <span class="math inline">\(p_2\)</span>, you can:</p>
<ul>
<li>take a SRS of size <span class="math inline">\(n_1\)</span> from the first population;</li>
<li>compute the sample proportion <span class="math inline">\(\hat{p}_1\)</span>, the number of yesses in this sample divided by <span class="math inline">\(n_1\)</span>;</li>
<li>take an independent SRS of size <span class="math inline">\(n_2\)</span> from the second population;</li>
<li>compute the sample proportion <span class="math inline">\(\hat{p}_2\)</span>, the number of yesses in this sample divided by <span class="math inline">\(n_2\)</span>;</li>
<li>subtract: <span class="math inline">\(\hat{p}_1 - \hat{p}_2\)</span>. This difference is your estimator for <span class="math inline">\(p_1-p_2\)</span>.</li>
</ul>
<p>The EV of <span class="math inline">\(\hat{p}_1 - \hat{p}_2\)</span> is:</p>
<p><span class="math display">\[EV(\hat{p}_1-\hat{p}_2)=p_1-p_2.\]</span></p>
<p>The SD of <span class="math inline">\(\hat{p}_1 - \hat{p}_2\)</span> is:</p>
<p><span class="math display">\[SD(\hat{p}_1 - \hat{p}_2) = \sqrt{\frac{p_1(1-p_1)}{n_1}+\frac{p_2(1-p_2)}{n_2}}.\]</span></p>
</div>
<div id="estimating-the-mean-of-differences" class="section level3">
<h3><span class="header-section-number">8.4.5</span> Estimating the Mean of Differences</h3>
<p>You estimate <span class="math inline">\(\mu_d\)</span> for a population by taking a SRS of size <span class="math inline">\(n\)</span>, say, and computing the <span class="math inline">\(\bar{d}\)</span>, the sample mean of the differences. The EV of <span class="math inline">\(\bar{d}\)</span> is <span class="math inline">\(\mu_d\)</span>, and the SD of <span class="math inline">\(\bar{d}\)</span> is:</p>
<p><span class="math display">\[SD(\bar{d})=\frac{\sigma_d}{\sqrt{n}},\]</span></p>
<p>where <span class="math inline">\(\sigma_d\)</span> is the standard deviation of all of the differences in the entire population.</p>
</div>
<div id="properties-of-ev-and-sd-of-estimators" class="section level3">
<h3><span class="header-section-number">8.4.6</span> Properties of EV and SD of Estimators</h3>
<div id="unbiased-estimators" class="section level4">
<h4><span class="header-section-number">8.4.6.1</span> Unbiased Estimators</h4>
<p>You have probably observed that, for each of the Basic Five, the EV of the estimator is exactly the parameter you are trying to estimate! When the EV of an estimator equals the target parameter, then the estimator is said to be <em>unbiased.</em> The estimators for the Basic Five are unbiased, but not all estimators are unbiased.</p>
</div>
<div id="sample-size-and-sd" class="section level4">
<h4><span class="header-section-number">8.4.6.2</span> Sample Size and SD</h4>
<p>Probably you have also noticed that for the SD of each of the Basic Five estimators, there are samples sizes in the denominators:</p>
<ul>
<li><span class="math inline">\(n\)</span> for one mean, one proportion, and the mean of differences;</li>
<li><span class="math inline">\(n_1\)</span> and <span class="math inline">\(n_2\)</span> for the difference of two means, and for the difference of two proportions.</li>
</ul>
<p>Now when the denominator of a fraction gets bigger, the fraction gets smaller, for example:</p>
<ul>
<li>1/10 = 0.1,</li>
<li>1/100 = 0.01,</li>
<li>1/1000 = 0.001, and so on.</li>
</ul>
<p>This observation leads to the following important point:</p>
<blockquote>
<p><em>For means and proportions, the larger the sample size the smaller the SD of the estimator will be.</em></p>
</blockquote>
<p>In other words:</p>
<blockquote>
<p><em>The larger the sample size, the less chance variation there is liable to be.</em></p>
</blockquote>
<p>This makes sense intuitively, as well. You might want to revisit the <code>SimpleRandom()</code> app, and this time vary the sample size.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">require</span>(manipulate)
<span class="kw">SimpleRandom</span>()</code></pre></div>
<p>You will notice that at large sample sizes the estimators tend to approximate their target parameters quite closely. This is due to the fact that at large sample sizes their SDs are small.</p>
<p><strong>Important Note!!</strong> The formulas for SD of the Basic Five Estimators are all based on taking a <em>simple random sample</em> from the population. For other probability sampling methods—such as stratified sampling, cluster sampling, and systematic sampling—the formulas are different and more complicated. Also, even for simple random sampling the formulas we have given are only approximately right. See the GeekNotes for the full story!</p>
</div>
</div>
</div>
<div id="estimators-shape-of-the-distribution" class="section level2">
<h2><span class="header-section-number">8.5</span> Estimators: Shape of the Distribution</h2>
<p>For each of the Basic Five estimators, we can now say two things about its distribution:</p>
<ul>
<li>We can specify the center (it’s the EV);</li>
<li>We can specify the spread (it’s the SD).</li>
</ul>
<p>When we describe a distribution, we also want to describe its <em>shape.</em> What are the shapes of these estimators? The following sequence of apps may help you to formulate a partial answer to this question.</p>
<p>The first app deals with sampling from one population, in order to estimate one proportion. For a fixed sample size, take samples one by one, and watch the density plot of the sample proportions take shape.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">require</span>(manipulate)
<span class="kw">PropSampler</span>(~cappun,<span class="dt">data=</span>imagpop)</code></pre></div>
<p>Try the app first for a small sample size. Ask yourself these questions:</p>
<ul>
<li>What is the shape, roughly:
<ul>
<li>Unimodal or bimodal?</li>
<li>Symmetric or skewed?</li>
</ul></li>
</ul>
<p>Try a much larger sample size, and ask yourself the same question.</p>
<p>The second app deals with sampling from one population, in order to estimate one mean:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">MeanSampler</span>(~income,<span class="dt">data=</span>imagpop)</code></pre></div>
<p>Again for a fixed sample size take samples one by one, and watch the density plot of the sample take shape. Try at least three samples sizes, including <span class="math inline">\(n=1\)</span> and <span class="math inline">\(n=30\)</span>. At each sample sizes, ask yourself the same questions about shape as before:</p>
<ul>
<li>What is the shape, roughly:
<ul>
<li>Unimodal or bimodal?</li>
<li>Symmetric or skewed?</li>
</ul></li>
</ul>
<p>The third app moves a bit more quickly. For any numerical variable you choose, and for any sample size you choose, the computer will draw 1000 simple random samples, compute the mean of each sample, and make a histogram of the results.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">SampDistMean</span>(imagpop)</code></pre></div>
<p>Try the same sample size several times. Then change the sample size, and try it a few more times. Keep changing sample sizes. Then do the whole thing over, for a new numerical variable. Is there a pattern to what you have observed?</p>
<p>The fourth app is similar, except that it deals with estimating the difference of two means:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">require</span>(manipulate)
<span class="kw">SampDist2Means</span>(imagpop)</code></pre></div>
<p>Again, play with sample sizes and with the various combinations of variables, and think a bit about what you noticed.</p>
<p>Finally, an app that that explores estimation of the difference between two proportions:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">require</span>(manipulate)
<span class="kw">SampDist2Props</span>(~sex+cappun,<span class="dt">data=</span>imagpop)</code></pre></div>
<p>Try it as before, but also vary the combination of variables. (For example, try <code>~sex+math</code>.) Again, think about what you notice.</p>
<blockquote>
<p><strong>Before you go any further, make sure you have thought carefully about any patterns you might have observed.</strong></p>
</blockquote>
<p>One of the things that you may have noticed is that no matter how the underlying population is distributed, the distribution of each of the Basic Five estimators looks more and more “bell-shaped” as the sample size increases. This phenomena comes from the famous:</p>
<blockquote>
<p><strong>Central Limit Theorem</strong>: For any population with a finite mean <span class="math inline">\(\mu\)</span> and finite standard deviation <span class="math inline">\(\sigma\)</span>, the distribution of the sample mean <span class="math inline">\(\bar{x}\)</span> gets closer and closer to</p>
</blockquote>
<p><span class="math display">\[norm(\mu,\frac{\sigma}{\sqrt{n}})\]</span></p>
<blockquote>
<p>as the sample size <span class="math inline">\(n\)</span> gets larger and larger.</p>
</blockquote>
<p>Although the Central Limit Theorem applies directly to the sample mean only, it can be used to show that each of the other Basic Five Estimators looks normally distributed, at “large enough” sample sizes. A sample proportion, for example is like a mean, because it involves dividing by the sample size <span class="math inline">\(n\)</span>. Hence <span class="math inline">\(\hat{p}\)</span> is normal-looking, if <span class="math inline">\(n\)</span> is big enough. The difference of two sample means will also look normal, because the difference of two independent normal random variables is also normal. The same idea helps us to see that the difference of two sample proportions will also be normal, when both samples sizes are large.</p>
<p>How large does <span class="math inline">\(n\)</span> have to be to be “large enough”? For most populations you encounter:</p>
<ul>
<li><span class="math inline">\(n \ge 30\)</span> is big enough for <span class="math inline">\(\bar{x}\)</span> to look normal;</li>
<li><span class="math inline">\(n_1 \ge 30\)</span> and <span class="math inline">\(n_2 \ge 30\)</span> is enough for <span class="math inline">\(\bar{x}_1 - \bar{x}_2\)</span> to look normal;</li>
<li><span class="math inline">\(n \ge 30\)</span> is big enough for <span class="math inline">\(\bar{d}\)</span> to look normal;</li>
<li>we saw back in Chapter 7 that when <span class="math inline">\(np \ge 10\)</span> and <span class="math inline">\(n(1-p) \ge 10\)</span> then <span class="math inline">\(\hat{p}\)</span> looks normal;</li>
<li>for <span class="math inline">\(\hat{p}_1-\hat{p}_2\)</span> to look normal then we only need:
<ul>
<li><span class="math inline">\(np_1 \ge 10\)</span>,</li>
<li><span class="math inline">\(n(1-p_1) \ge 10\)</span>,</li>
<li><span class="math inline">\(np_2 \ge 10\)</span>, and</li>
<li><span class="math inline">\(n(1-p_2) \ge 10\)</span>.</li>
</ul></li>
</ul>
<p>The more skewed the population is, the larger the sample size needs to be before the distribution of <span class="math inline">\(\bar{x}\)</span> looks normal—you may have noticed this when you were playing with the apps. Nevertheless, we don’t often run across a population that is so skewy that the sample size <span class="math inline">\(n=30\)</span> is still “too small”. Also, if the underlying population is pretty close to bell-shaped, then the distribution of <span class="math inline">\(\bar{x}\)</span> will be approximately normal, even when the sample size <span class="math inline">\(n\)</span> is quite small. (You may have noticed that in the apps, too!)</p>
</div>
<div id="probability-for-estimators" class="section level2">
<h2><span class="header-section-number">8.6</span> Probability for Estimators</h2>
<p>Imagine that you are a very powerful being: you can find out everything about the present world that you would like to know, all in a flash. But you are not all-knowing: you cannot know the future. Hence you are not God—-maybe you are more like Zeus or Athena, or one of the other deities from Mount Olympus.</p>
<p>Since you can find out anything you want about the present world, you can know everything about a population. In particular you can know any population parameter that you like.</p>
<p>Now imagine that you are looking down from Mount Olympus, watching a poor statistician—a mere mortal—about to take a random sample from some population about which you know everything. Since you can’t know the future, you don’t know what his or her sample will contain, so you don’t know what the values of any of the estimators will be. But since you know the population, you know the <em>distribution</em> of the estimators: you known mean, the SD and the shape of each estimator.</p>
<p>Armed with this knowledge, you can answer all sorts of <em>probability</em> questions about an estimator: that is, you can determine how likely it is that the estimator will fall within any given range. Let’s try a few examples of this.</p>
<blockquote>
<p><strong>Example (1):</strong> A statistician is about to take a SRS of size <span class="math inline">\(n=25\)</span> from <code>imagpop</code> and compute <span class="math inline">\(\bar{x}\)</span>, the sample mean of the heights of the 25 selected individuals. What is the probability that the sample mean will exceed 68.3 inches? In other words, what is:</p>
</blockquote>
<p><span class="math display">\[P(\bar{x} &gt; 68.3)?\]</span></p>
<p><strong>Solution:</strong> First of all, we use our god-like powers to find the mean <span class="math inline">\(\mu\)</span> and the standard deviation <span class="math inline">\(\sigma\)</span> of the heights in the population:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">favstats</span>(~height,<span class="dt">data=</span>imagpop)[<span class="dv">6</span>:<span class="dv">7</span>]</code></pre></div>
<pre><code>##      mean       sd
##  67.53012 3.907014</code></pre>
<p>Now the distribution of <span class="math inline">\(\bar{x}\)</span> is probably very close to normal. That’s because the underlying population of heights is already pretty bell-shaped. We know this because we can use our god-like powers to draw a density plot of the entire population:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">densityplot</span>(~height,<span class="dt">data=</span>imagpop,
            <span class="dt">xlab=</span><span class="st">&quot;Height (inches)&quot;</span>,
            <span class="dt">main=</span><span class="st">&quot;Imagpop Heights&quot;</span>,
            <span class="dt">plot.points=</span><span class="ot">FALSE</span>)</code></pre></div>
<div class="figure"><span id="fig:imagpoopheightdensity"></span>
<img src="bookdown-demo_files/figure-html/imagpoopheightdensity-1.png" alt="Imagpop Heights.  Height in this population is roughly normall distibuted." width="672" />
<p class="caption">
Figure 8.2: Imagpop Heights. Height in this population is roughly normall distibuted.
</p>
</div>
<p>The results appear in Figure [Imagpop Heights]. Indeed, the distribution of <strong>height</strong> is quite bell-shaped. Therefore, even though the intended sample size (<span class="math inline">\(n=25\)</span>) is a bit lower than the suggested “safe” level of 30, <span class="math inline">\(\bar{x}\)</span> should be quite normal, with mean 67.53 inches and SD equal to</p>
<p><span class="math display">\[\frac{\sigma}{\sqrt{n}}= \frac{3.907}{\sqrt{25}} \approx 0.78.\]</span></p>
<p>In symbols, we might say:</p>
<p><span class="math display">\[\bar{x} \sim norm(67.53,0.78).\]</span></p>
<p>So in order to find the probability we can just ask R to tell us the area under the appropriate normal curve after 68.3:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">pnormGC</span>(<span class="fl">68.3</span>,<span class="dt">region=</span><span class="st">&quot;above&quot;</span>,
        <span class="dt">mean=</span><span class="fl">67.53</span>,<span class="dt">sd=</span><span class="fl">0.78</span>)</code></pre></div>
<pre><code>## [1] 0.1617773</code></pre>
<p>So there is about a 16.2% chance that <span class="math inline">\(\bar{x}\)</span> will exceed 68.3 inches.</p>
<blockquote>
<p><strong>Example (2):</strong> A statistician plans to take a SRS of 30 males from the population of all males in <code>imagpop</code>, and an independent SRS of 40 females from the population of all women in <code>imagpop</code>. She will then compute <span class="math inline">\(\bar{x}_1-\bar{x}_2\)</span>, the sample mean height of the males minus the sample mean height of the females. Approximately what is the chance that the difference of sample means will be between 4 and 6 inches?</p>
</blockquote>
<p><strong>Solution</strong>: This time the samples sizes are <span class="math inline">\(n_1=30\)</span> and <span class="math inline">\(n_2=40\)</span>. Both sample sizes are fairly large, so we can use the Central Limit Theorem to conclude that <span class="math inline">\(\bar{x}_1-\bar{x}_2\)</span> is approximately normal.</p>
<p>Next, we need to compute the EV and SD of <span class="math inline">\(\bar{x}_1-\bar{x}_2\)</span>. For this we will need means and standard deviations of of the heights for:</p>
<ul>
<li>all males in <code>imagpop</code>, and</li>
<li>all females in <code>imagpop</code>.</li>
</ul>
<p>Hence we ask for:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">favstats</span>(height~sex,<span class="dt">data=</span>imagpop)[<span class="dv">6</span>:<span class="dv">7</span>]</code></pre></div>
<pre><code>##    max     mean
## 1 76.2 64.99624
## 2 80.2 70.03178</code></pre>
<p>So the EV of <span class="math inline">\(\bar{x}_1-\bar{x}_2\)</span> is</p>
<p><span class="math display">\[70.03-65=5.03\]</span></p>
<p>inches, and the SD of <span class="math inline">\(\bar{x}_1-\bar{x}_2\)</span> is</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">sqrt</span>(<span class="fl">3.013</span>^<span class="dv">2</span>^<span class="dv">2</span>/<span class="dv">30</span><span class="fl">+2.962</span>^<span class="dv">2</span>/<span class="dv">40</span>)</code></pre></div>
<pre><code>## [1] 1.722336</code></pre>
<p>So <span class="math inline">\(\bar{x}_1-\bar{x}_2\)</span> is approximately <span class="math display">\[norm(5.03,1.72).\]</span></p>
<p>Now we can get the desired probability:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">pnormGC</span>(<span class="kw">c</span>(<span class="dv">4</span>,<span class="dv">6</span>),<span class="dt">region=</span><span class="st">&quot;between&quot;</span>
        ,<span class="dt">mean=</span><span class="fl">5.03</span>,<span class="dt">sd=</span><span class="fl">1.72</span>)</code></pre></div>
<pre><code>## [1] 0.4389664</code></pre>
<p>So there is about a 43.9% chance that <span class="math inline">\(\bar{x}_1-\bar{x}_2\)</span> will turn out to be between 4 and 6 inches. That is, there is about a 43.9% chance that the sample guys will be, on average, between 4 and 6 inches taller than the gals, on average.</p>
<blockquote>
<p><strong>Example (3):</strong> A certain roughly normal population has a mean height of 65 inches, with a standard deviation of 3 inches.</p>
</blockquote>
<blockquote>
<p>(1). You plan to select one individual at random from the population. Fill in the blanks, and explain: <em>There is about a 68% chance that the selected individual will be between ____ and ___ inches tall.</em></p>
</blockquote>
<blockquote>
<p>(2). You plan to select 16 individuals from the population by simple random sampling. Fill in the blanks, and explain: <em>There is about 95% chance that the mean height of the selected individuals will be between ____ and ____.</em></p>
</blockquote>
<p><strong>Answer to 1:</strong> The number 68 should remind you of the “68” part of the 68-95 Rule for Random Variables:</p>
<blockquote>
<p><em>When the distribution of a random variable is bell-shaped, there is about a 68% chance that the random variable will land within one SD of it’s EV.</em></p>
</blockquote>
<p>The EV of the height of a single randomly-selected person is just <span class="math inline">\(\mu\)</span>, the population mean, so the EV of this random adult is 65 inches. The SD for the height of the person is the SD for the population, and that’s 3. (You could also think of the height of one random person as the sample mean for a sample of size 1. Then the SD of this sample mean is <span class="math inline">\(\sigma/\sqrt(1) = 3/1 = 3\)</span>.)</p>
<p>Therefore, by the 68-95 Rule for Random Variables, there is about a 68% chance that this randomly selected person is between <span class="math inline">\(65-3=62\)</span> and <span class="math inline">\(65+3=68\)</span> inches. So we fill in the blanks with 62 and 68.</p>
<p><strong>Answer to 2:</strong> When you sample <span class="math inline">\(n=16\)</span> individuals, the EV and SD of <span class="math inline">\(\bar{x}\)</span> are:</p>
<p><span class="math display">\[EV(\bar{x})=\mu=65, \\
SD(\bar{x})=3/\sqrt{16}=0.75.\]</span></p>
<p>By the “95” part of the 68-95 Rule, the approximately normal random variable <span class="math inline">\(\bar{x}\)</span> has about a 95% chance of landing within two SDs of its EV, so we should fill in the blanks with <span class="math inline">\(65-2(0.75)=63.5\)</span> and <span class="math inline">\(65+2(0.75)=66.5\)</span>.</p>
<blockquote>
<p><strong>Example (4):</strong> Suppose that forty-five percent of all registered voters approve of the job that the President of the U.S. is doing. Approximately what is the probability that, in a random sample of 1600 voters, between 43% and 47% will approve of the job that the President is doing?</p>
</blockquote>
<p><strong>Solution 1:</strong> We could turn this into a problem from Chapter 7. The <em>number</em> <span class="math inline">\(X\)</span> of voters in the sample who approve is a binomial random variable, with the number of trials <span class="math inline">\(n\)</span> is set at 1600 and with a probability of success on each trial equal to 0.45. Also:</p>
<ul>
<li>43% of 1600 = 0.43 x 1600 = 688;</li>
<li>47% of 1600 = 0.47 x 1600 = 752.</li>
</ul>
<p>So we are really asking for: <span class="math inline">\(P(688 \le X \le 752).\)</span> We can get this with R:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">pbinomGC</span>(<span class="kw">c</span>(<span class="dv">688</span>,<span class="dv">752</span>),<span class="dt">region=</span><span class="st">&quot;between&quot;</span>,
         <span class="dt">size=</span><span class="dv">1600</span>,<span class="dt">prob=</span><span class="fl">0.45</span>)</code></pre></div>
<pre><code>## [1] 0.8976011</code></pre>
<p>So there is about an 89.7% chance that the sample proportion <span class="math inline">\(\hat{p}\)</span> will be between 0.43 and 0.47.</p>
<p><strong>Solution 2:</strong> This time we will use the normal approximation for <span class="math inline">\(\hat{p}\)</span>. It is certainly safe to do so, because for us:</p>
<ul>
<li><span class="math inline">\(np=1600(0.45)=720 \ge 10\)</span>, and</li>
<li><span class="math inline">\(n(1-p)=100(1-0.45)=880 \ge 10.\)</span></li>
</ul>
<p>Now the EV for <span class="math inline">\(\hat{p}\)</span> is <span class="math inline">\(p\)</span>, which is 0.45. As for the SD of <span class="math inline">\(\hat{p}\)</span>, it is:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">sqrt</span>(<span class="fl">0.45</span>*(<span class="dv">1</span><span class="fl">-0.45</span>)/<span class="dv">1600</span>)</code></pre></div>
<pre><code>## [1] 0.01243734</code></pre>
<p>By the normal approximation, the answer we seek is:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">pnormGC</span>(<span class="kw">c</span>(<span class="fl">0.43</span>,<span class="fl">0.47</span>),<span class="dt">region=</span><span class="st">&quot;between&quot;</span>,
        <span class="dt">mean=</span><span class="fl">0.45</span>,<span class="dt">sd=</span><span class="fl">0.01243</span>)</code></pre></div>
<pre><code>## [1] 0.8923859</code></pre>
<blockquote>
<p><strong>Example (5):</strong> 50% of all OSU students favor stricter gun laws, and 40% of all UK students favor stricter gun laws. A statistician plans to take a SRS of 400 OSU students and an independent SRS of 900 UK students, and to ask the sample students if they favor stricter gun laws. Fill in the blanks, and explain: there is about a 95% chance that the difference in sample proportions (OSU minus UK) will be between _____ and _____.</p>
</blockquote>
<p><strong>Solution:</strong> First we find the EV and the SD of <span class="math inline">\(\hat{p}_1-\hat{p}_2\)</span>. The EV is just</p>
<p><span class="math display">\[p_1-p_2=0.50-0.40=0.10,\]</span> and the SD is:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">sqrt</span>(<span class="fl">0.50</span>*(<span class="dv">1</span><span class="fl">-0.50</span>)/<span class="dv">400</span><span class="fl">+0.40</span>*(<span class="dv">1</span><span class="fl">-0.40</span>)/<span class="dv">900</span>)</code></pre></div>
<pre><code>## [1] 0.02986079</code></pre>
<p>Next, we note that the distribution of <span class="math inline">\(\hat{p}_1-\hat{p}_2\)</span> is approximately normal. This is because:</p>
<ul>
<li><span class="math inline">\(n_1p_1 = (400)(0.50) = 200 \ge 10;\)</span></li>
<li><span class="math inline">\(n_1(1-p_1) = (400)(0.50) = 200 \ge 10;\)</span></li>
<li><span class="math inline">\(n_2p_2 = (900)(0.40) = 360 \ge 10;\)</span></li>
<li><span class="math inline">\(n_2(1-p_2) = (900)(0.60) = 540 \ge 10.\)</span></li>
</ul>
<p>By the 68-95 Rule, there is about a 95% chance that the approximately-normal <span class="math inline">\(\hat{p}_1-\hat{p}_2\)</span> will land within 2 SDs of its EV. So we fill in the blanks as follows:</p>
<ul>
<li>The blank for the lower bound is:</li>
</ul>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="fl">0.10</span><span class="dv">-2</span>*<span class="fl">0.02986</span></code></pre></div>
<pre><code>## [1] 0.04028</code></pre>
<p>or about 4.03%.</p>
<ul>
<li>The blank for the upper bound is:</li>
</ul>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="fl">0.10</span><span class="dv">+2</span>*<span class="fl">0.02986</span></code></pre></div>
<pre><code>## [1] 0.15972</code></pre>
<p>or about 15.97%.</p>
</div>
<div id="inference-with-estimators" class="section level2">
<h2><span class="header-section-number">8.7</span> Inference With Estimators</h2>
<div id="we-are-not-deities" class="section level3">
<h3><span class="header-section-number">8.7.1</span> We Are Not Deities</h3>
<p>So far in this chapter, we have been imagining that we are deities who somehow can know everything about a population. This knowledge, together with a little statistical theory, allowed us to answer questions concerning the chances for a sample from the population to take on certain given values.</p>
<p>But now let’s turn it around, and take the point of view of a statistician, a mere mortal. Such a person does NOT know everything about the population. However, she has taken a sample from the population, and she can compute any number that is based on her sample. She would like to use statistics from her sample to estimate parameters in the population.</p>
<p>This statisticians has two primary questions:</p>
<blockquote>
<ol style="list-style-type: decimal">
<li>Based on my sample, what’s the best single guess at the population parameter?</li>
<li>By how much is my best guess liable to differ from the actual value of the population parameter?</li>
</ol>
</blockquote>
<p>In the case of the Basic Five, the answer to Question 1 should be fairly clear: your best guess at a Basic Five parameter is the <em>estimator</em> for that parameter: e.g., your best guess at a population mean <span class="math inline">\(\mu\)</span> is the sample mean <span class="math inline">\(\bar{x}\)</span>, and so on for the other four members of the Basic Five.</p>
<p>How about Question #2? This is a bit tougher. The statistician has the same access to statistical theory that a deity has: she knows that the estimator is liable to differ from the parameter by an SD or so. However, the formulas for the SDs all involve population parameters, so she cannot compute the numerical value of the SD.</p>
<p>For example, the SD of <span class="math inline">\(\bar{x}\)</span> is <span class="math inline">\(\frac{\sigma}{\sqrt{n}}\)</span>. The statistician knows <span class="math inline">\(n\)</span>, the sample size, but she does NOT know <span class="math inline">\(\sigma\)</span>, the SD of the population. Hence it would seem that she cannot say anything about <em>how much</em> her estimate <span class="math inline">\(\bar{x}\)</span> is liable to differ from the the target parameter <span class="math inline">\(\mu\)</span>.</p>
<p>There is a way around this, however. Since the statistician has the sample she can compute the <em>sample standard deviation</em> <span class="math inline">\(s\)</span>. Since <span class="math inline">\(s\)</span> is an estimate of <span class="math inline">\(\sigma\)</span>, it stands to reason that the quantity</p>
<p><span class="math display">\[\frac{s}{\sqrt{n}}\]</span></p>
<p>could serve as an estimate of</p>
<p><span class="math display">\[\frac{\sigma}{\sqrt{n}}.\]</span></p>
<p>The quantity <span class="math inline">\(\frac{s}{\sqrt{n}}\)</span> is known as the <em>standard error</em> of <span class="math inline">\(\bar{x}\)</span>, and it is often written <span class="math inline">\(SE(\bar{x})\)</span> for short.</p>
</div>
<div id="practice-with-estimator-and-se" class="section level3">
<h3><span class="header-section-number">8.7.2</span> Practice With Estimator and SE</h3>
<p>The SD for every Basic Five estimator has a corresponding SE. A mere mortal who knows only the sample can use the SE to estimate the SD of the estimator of the parameter of interest:</p>
<ul>
<li>One Mean: <span class="math inline">\(SE(\bar{x}) = \frac{s}{\sqrt{n}}\)</span>, where <span class="math inline">\(s\)</span> is the SD of the sample;</li>
<li>Difference of Two Means: <span class="math inline">\(SE(\bar{x}_1-\bar{x}_2) = \sqrt{\frac{s_1^2}{n_1}+\frac{s_2^2}{n_2}}\)</span>, where <span class="math inline">\(s_1\)</span> and <span class="math inline">\(s_2\)</span> are the SDs of the first and second sample, respectively;</li>
<li>One Proportion: <span class="math inline">\(SE(\hat{p})= \sqrt{\frac{\hat{p}(1-\hat{p})}{n}}\)</span>, where <span class="math inline">\(\hat{p}\)</span> is the sample proportion.</li>
<li>Difference of Two Proportions: <span class="math inline">\(SE(\hat{p}_1-\hat{p}_2)=\sqrt{\frac{\hat{p}_1(1-\hat{p}_1)}{n_1}+\frac{\hat{p}_2(1-\hat{p}_2)}{n_2}}\)</span>, where <span class="math inline">\(\hat{p}_1\)</span> and <span class="math inline">\(\hat{p}_2\)</span> are the two sample proportions.</li>
<li>Mean of Differences: <span class="math inline">\(SE(\bar{d})=\frac{s_d}{\sqrt{n}}\)</span>, where <span class="math inline">\(s_d\)</span> is the standard deviation of the differences in the sample.</li>
</ul>
<p>We will use the estimator and its SE together to say what we think the parameter is, and by how much we think our estimate might be off.</p>
<blockquote>
<p><strong>Example (1):</strong> Using the <code>mat111survey</code> data, estimate the mean height of the population of all GC students. Also, give a figure that indicates the amount by which your estimate might differ from the population mean.</p>
</blockquote>
<p><strong>Solution:</strong> We are interested in one population mean:</p>
<blockquote>
<p><span class="math inline">\(\mu\)</span> = mean height of all GC students.</p>
</blockquote>
<p>We therefore want <span class="math inline">\(\bar{x}\)</span> and <span class="math inline">\(SE(\bar{x})\)</span>. For <span class="math inline">\(SE(\bar{x})\)</span>, we will need the SD of the sample, and we will need <span class="math inline">\(n\)</span>, the sample size. So we really need three things, and we can get them quickly with <strong>favstats</strong>:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">favstats</span>(~height,<span class="dt">data=</span>m111survey)[<span class="dv">6</span>:<span class="dv">8</span>]</code></pre></div>
<pre><code>##      mean       sd  n
##  67.98662 5.296414 71</code></pre>
<p>So:</p>
<ul>
<li><span class="math inline">\(\bar{x} = 67.987\)</span>,</li>
<li><span class="math inline">\(s = 5.296\)</span>,</li>
<li><span class="math inline">\(n = 71\)</span>.</li>
</ul>
<p>Now we can get <span class="math inline">\(SE(\bar{x})\)</span> by applying the formula:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="fl">5.296</span>/<span class="kw">sqrt</span>(<span class="dv">71</span>)</code></pre></div>
<pre><code>## [1] 0.6285196</code></pre>
<p>At last, we can answer the question that was originally asked: <em>The mean height of all GC students is about 67.99 inches, give or take 0.63 inches or so.</em></p>
<blockquote>
<p><strong>Example (2)</strong>: In a random sample of 2500 adults in the year 1995, 70% said they favored the death penalty. An independent random sample of 1600 adults in the year 2013 had 55% percent of them favoring the death penalty. Fill in the blanks, and explain: The difference in the proportions of adults who favor the death penalty (Year 1995 minus Year 2013) is about _______, give or take _______ or so.</p>
</blockquote>
<p><strong>Solution</strong>: It’s easier to answer this sort of question using summary data than with raw data (like we had in the previous example). We just need to plug into the formulas for the estimator and the SE of the estimator. The parameter of interest is a difference of two proportions, so we just plug into the formulas.</p>
<p>The estimator <span class="math inline">\(\hat{p}_1\)</span> and <span class="math inline">\(\hat{p}_2\)</span> is:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="fl">0.70-0.55</span></code></pre></div>
<pre><code>## [1] 0.15</code></pre>
<p>That’s 15%. Now for the SE of <span class="math inline">\(\hat{p}_1-\hat{p}_2\)</span>:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">sqrt</span>(<span class="fl">0.70</span>*(<span class="dv">1</span><span class="fl">-0.70</span>)/<span class="dv">2500</span>)+<span class="fl">0.55</span>*(<span class="dv">1</span><span class="fl">-0.55</span>)/<span class="dv">1600</span></code></pre></div>
<pre><code>## [1] 0.009319839</code></pre>
<p>This is about 0.93%.</p>
<p>Now we can fill in the blanks: <em>The difference in the percentage of adults who favor the death penalty (Year 1995 minus Year 2013) is about 15%, give or 0.93% or so.</em></p>
</div>
<div id="the-68-95-rule-for-estimation" class="section level3">
<h3><span class="header-section-number">8.7.3</span> The 68-95 Rule for Estimation</h3>
<p>Say that you are about to flip a fair coin. You don’t know how it will land, but you do know that there is a 50% chance that it will land Heads. Next, imagine that a friend has flipped a coin, and he is hiding it in his fist. You don’t know whether it’s Heads or Tails, but you feel 50% <em>confident</em> that it is Heads. Why? Because <em>before</em> it was flipped there was a 50% chance that it would land heads.</p>
<p>This idea about confidence carries over to more complicated random processes, including processes involving the random collection of data.</p>
<p>For example, say that you are about to take a large simple random sample from a population. Statistical theory, together with the 68-95 Rule, says that there is about a 68% chance that the sample mean <span class="math inline">\(\bar{x}\)</span> will land within one SE of the mean <span class="math inline">\(\mu\)</span> of the population. Therefore, if you have already taken a sample, you are justified in feeling 68% <em>confident</em> that <span class="math inline">\(\bar{x}\)</span> <em>actually did</em> land within one SE of <span class="math inline">\(\mu\)</span>. Another way of saying the same thing is that you feel 68% confident that <span class="math inline">\(\mu\)</span> lies within one SE of the <span class="math inline">\(\bar{x}\)</span> that you got.</p>
<p>We encapsulate this idea in the</p>
<blockquote>
<p><strong>68-95 Rule for Estimation</strong>: If an estimator for a population parameter has a roughly bell-shaped probability distribution, then:</p>
</blockquote>
<blockquote>
<ul>
<li>we can be about 68%-confident that the parameter is within one standard error of the estimator;</li>
</ul>
</blockquote>
<blockquote>
<ul>
<li>we can be about 95%-confident that the parameter is within two standard errors of the estimator;</li>
</ul>
</blockquote>
<blockquote>
<ul>
<li>we can be about 99.7%-confident that the parameter is within three standard errors of the estimator;</li>
</ul>
</blockquote>
<p>Here is an example:</p>
<blockquote>
<p><strong>Example:</strong> You sample 36 people at random from a population, and find that their mean height is 68 inches, and that the SD of their heights is 3 inches. Fill in the blanks: you can be about 95%-confident that the population mean <span class="math inline">\(\mu\)</span> is between _____ and _____.</p>
</blockquote>
<p>For you, the value of <span class="math inline">\(\bar{x}\)</span> is 68, and the SE of <span class="math inline">\(\bar{x}\)</span> is:</p>
<p><span class="math display">\[3/\sqrt{36} = 3/6 = 0.5\]</span></p>
<p>inches. So, two SEs is 1 inch.</p>
<p>Hence you can feel 95% confident that <span class="math inline">\(\mu\)</span>, the mean height of all people in the population, is within 1 inch of 68, that is, between 67 and 69 inches.</p>
<p>People say that the interval of real numbers from 67 to 69 inches is a <em>95%-confidence interval for <span class="math inline">\(\mu\)</span>.</em> Formulas for the confidence intervals we have produced using the 68-95 Rule for Estimation are often written out as follows:</p>
<ul>
<li>The “68” part of the Rule gives us the following approximately 68%-confidence interval for <span class="math inline">\(\mu\)</span>:</li>
</ul>
<p><span class="math display">\[\bar{x} \pm 1 \times SE(\bar{x}).\]</span></p>
<ul>
<li>The “95” part of the Rule gives us the following approximately 95%-confidence interval for <span class="math inline">\(\mu\)</span>:</li>
</ul>
<p><span class="math display">\[\bar{x} \pm 2 \times SE(\bar{x}).\]</span></p>
<ul>
<li>The “99.7” part of the Rule gives us the following approximately 99.7%-confidence interval for <span class="math inline">\(\mu\)</span>:</li>
</ul>
<p><span class="math display">\[\bar{x} \pm 3 \times SE(\bar{x}).\]</span></p>
<p>In each of the above formulas, the sample mean <span class="math inline">\(\bar{x}\)</span> — the estimator for <span class="math inline">\(\mu\)</span> — is smack in the middle of the confidence interval. The quantity after the <span class="math inline">\(\pm\)</span>-sign — the number you subtract to make the lower bound of the interval and add to make the upper bound — is called the <em>margin of error.</em> As you can see the margin of error is the product of two further numbers:</p>
<ul>
<li>the standard error <span class="math inline">\(SE(\bar{x})\)</span>, and</li>
<li>a number — 1,2 or 3 — that determines your level of confidence that <span class="math inline">\(\mu\)</span> actually lies inside the interval that you are forming. This number is called a <em>multiplier.</em></li>
</ul>
<p>The sample estimate determines the center and the margin of error determines the width of the confidence interval. Since the margin of error is determined by the multiplier and the standard error, we can see the role that sample size and confidence level play in the width of the confidence interval:</p>
<ul>
<li><p>The larger the sample size, <span class="math inline">\(n\)</span>, the smaller the SE. So, <em>larger sample sizes produce more narrower confidence intervals</em>.</p></li>
<li><p>The higher level of confidence we have that an interval contains the true parameter value, the bigger the multiplier will have to be. So, <em>higher confidence levels produce wider confidence intervals</em>.</p></li>
</ul>
<p>The confidence intervals formed by the 68-95 Rule for Estimation are somewhat “rough”, in the sense that they have only approximately the level of confidence that they advertise. This is due to two approximations:</p>
<ul>
<li>the 1,2 and 3 numbers give only roughly the areas under the normal curve that they claim. (For example, the area under the standard normal curve between -2 and 2 is a bit more than 0.95. If you want to capture 95% of the area, you should really look between -1.96 and 1.96.)</li>
<li>we replaced <span class="math inline">\(SD(\bar{x})\)</span> — which a working statistician would likely not know – with the approximation <span class="math inline">\(SE(\bar{x})\)</span> that she could actually compute from her sample.</li>
</ul>
<p>We will explore the idea of confidence intervals in more depth in the next Chapter: in particular, we will look for ways to make them less “rough.”</p>
</div>
</div>
<div id="summary-of-formulas" class="section level2">
<h2><span class="header-section-number">8.8</span> Summary of Formulas</h2>
<ul>
<li>For one mean <span class="math inline">\(\mu\)</span>:
<ul>
<li>Estimator is <span class="math inline">\(\bar{x}\)</span></li>
<li>EV is <span class="math inline">\(\mu\)</span></li>
<li>SD is <span class="math inline">\(\frac{\sigma}{\sqrt{n}}\)</span></li>
<li>SE is <span class="math inline">\(\frac{s}{\sqrt{n}}\)</span></li>
</ul></li>
<li>For the difference of two means <span class="math inline">\(\mu_1-\mu_2\)</span>:
<ul>
<li>Estimator is <span class="math inline">\(\bar{x}_1-\bar{x}_2\)</span></li>
<li>EV is <span class="math inline">\(\mu_1-\mu_2\)</span></li>
<li>SD is <span class="math inline">\(\sqrt{\frac{\sigma_1^2}{n_1}+\frac{\sigma_2^2}{n_2}}\)</span></li>
<li>SE is <span class="math inline">\(\sqrt{\frac{s_1^2}{n_1}+\frac{s_2^2}{n_2}}\)</span></li>
</ul></li>
<li>For one proportion <span class="math inline">\(p\)</span>:
<ul>
<li>Estimator is <span class="math inline">\(\hat{p}\)</span></li>
<li>EV is <span class="math inline">\(p\)</span></li>
<li>SD is <span class="math inline">\(\sqrt{\frac{p(1-p)}{n}}\)</span></li>
<li>SE is <span class="math inline">\(\sqrt{\frac{\hat{p}(1-\hat{p})}{n}}\)</span></li>
</ul></li>
<li>For the difference of two proportions <span class="math inline">\(p_1-p_2\)</span>:
<ul>
<li>Estimator is <span class="math inline">\(\hat{p}_1-\hat{p}_2\)</span></li>
<li>EV is <span class="math inline">\(p_1-p_2\)</span></li>
<li>SD is <span class="math inline">\(\sqrt{\frac{p_1(1-p_1)}{n_1}+\frac{p_2(1-p_2)}{n_2}}\)</span></li>
<li>SE is <span class="math inline">\(\sqrt{\frac{\hat{p}_1(1-\hat{p}_1)}{n_1}+\frac{\hat{p}_2(1-\hat{p}_2)}{n_2}}\)</span></li>
</ul></li>
<li>For the mean of differences <span class="math inline">\(\mu_d\)</span>:
<ul>
<li>Estimator is <span class="math inline">\(\bar{d}\)</span></li>
<li>EV is <span class="math inline">\(\mu_d\)</span></li>
<li>SD is <span class="math inline">\(\frac{\sigma_d}{\sqrt{n}}\)</span></li>
<li>SE is <span class="math inline">\(\frac{s_d}{\sqrt{n}}\)</span></li>
</ul></li>
</ul>
</div>
<div id="thoughts-on-r" class="section level2">
<h2><span class="header-section-number">8.9</span> Thoughts on R</h2>
<p>In this chapter:</p>
<ul>
<li>You have really seen how useful it can be to use R as a calculator.</li>
<li>You have also got a lot more practice with <code>pnormGC()</code> and a bit more practice with <code>pbinomGC()</code>. It’s important to become very familiar with these functions.</li>
</ul>
<p>You have also seen some examples in which we use <code>favstats()</code>, but only need to see a few columns of the output. For example, to see only columns 6 and 7 (the columns that give the mean and the SD), you simply type:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">favstats</span>(Formula,<span class="dt">data=</span>MyData)[<span class="dv">6</span>:<span class="dv">7</span>]</code></pre></div>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="basic-probability.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="introduction-1.html" class="navigation navigation-next " aria-label="Next page""><i class="fa fa-angle-right"></i></a>

<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"facebook": true,
"twitter": true,
"google": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/rstudio/bookdown-demo/edit/master/08-ProbInSamp.Rmd",
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    if (location.protocol !== "file:" && /^https?:/.test(script.src))
      script.src  = script.src.replace(/^https?:/, '');
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
